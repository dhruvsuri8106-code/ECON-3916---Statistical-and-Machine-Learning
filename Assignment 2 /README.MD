# Audit 02: Deconstructing Statistical Lies

This audit examined how statistical metrics can create misleading conclusions when underlying assumptions, distributions, or sampling mechanisms are flawed. Through controlled simulations and statistical diagnostics, three major sources of analytical distortion were identified: latency skew, false positives under low base rates, and survivorship bias in heavy-tailed financial systems. Each case demonstrates how technically correct statistics can still produce fundamentally incorrect interpretations.

The first finding involved latency skew in system performance data. Latency distributions were heavily right-skewed, meaning that while most requests completed quickly, a small number of extreme slow events dramatically inflated the average. When the mean latency was used as the primary performance metric, it overstated the typical user experience. However, when robust statistics such as the median and Median Absolute Deviation (MAD) were applied, the central tendency and variability were far more stable and representative. This demonstrated that the mean is highly sensitive to outliers in heavy-tailed distributions and can mislead engineers into believing systems are less stable than they truly are—or worse, hide instability if outliers are ignored. The key conclusion is that median-based metrics provide a more reliable measure of real-world system performance.

The second finding involved the false positive paradox in classification systems. A simulated plagiarism detector with 98% sensitivity and 98% specificity initially appeared highly reliable. However, when applied to a population where cheating was rare (a base rate of 0.1%), the posterior probability that a flagged student was actually cheating was only approximately 4.7%. This revealed that most flagged cases were false positives, despite the detector’s high accuracy. The discrepancy arose because accuracy metrics do not account for base rates. When the underlying event is rare, even highly accurate systems produce a majority of incorrect alerts. This demonstrated that accuracy alone is an insufficient measure of model reliability and that posterior probabilities must be evaluated using Bayes’ Theorem. The key conclusion is that model performance must be interpreted in the context of base rates, not just sensitivity and specificity.

The third finding demonstrated survivorship bias in crypto markets through simulation of 10,000 token launches using a power-law (Pareto) distribution. This distribution reflected real financial markets, where most tokens fail or remain near zero value, while a small number achieve extreme valuations. When the mean peak market cap was calculated across all tokens, it was relatively modest. However, when only the top 1% of tokens—the “survivors”—were analyzed, the average market cap increased dramatically. This created the illusion that typical token performance was far higher than reality. The bias occurred because failed tokens were excluded from the analysis, leaving only successful outcomes. This demonstrated how survivorship bias inflates perceived performance and distorts investor expectations. The key conclusion is that analyzing only surviving entities produces systematically biased conclusions and misrepresents true outcome distributions.

Overall, this audit demonstrated that statistical conclusions are highly sensitive to distribution shape, base rates, and sampling selection. Mean-based metrics fail under skewed distributions, accuracy metrics fail under rare-event conditions, and survivor-only sampling produces inflated performance estimates. In each case, the statistical calculations themselves were correct, but the interpretation was misleading due to improper contextual framing. The broader implication is that statistical literacy requires not only computing metrics but also understanding when those metrics fail.
